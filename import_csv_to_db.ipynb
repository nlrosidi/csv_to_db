{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of import_csv_to_db-v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Cg4iHtgXqxjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Import CSVs to DB\n",
        "\n",
        "Script to upload CSVs to a postgres database\n"
      ]
    },
    {
      "metadata": {
        "id": "nrx0LXyYqA3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install psycopg2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWApQY4NCID4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ]
    },
    {
      "metadata": {
        "id": "sTlsgh_HuzXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def csv_files():\n",
        "\n",
        "    #get names of only csv files\n",
        "    csv_files = []\n",
        "    for file in os.listdir(os.getcwd()):\n",
        "        if file.endswith(\".csv\"):\n",
        "            csv_files.append(file)\n",
        "\n",
        "    return csv_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIjq5DcexgOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def configure_dataset_directory(csv_files, dataset_dir):\n",
        "  \n",
        "    #make dataset folder to process csv files\n",
        "    try: \n",
        "      mkdir = 'mkdir {0}'.format(dataset_dir)\n",
        "      os.system(mkdir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    #move csv files to dataset folder\n",
        "    for csv in csv_files:\n",
        "      mv_file = 'mv {0} {1}'.format(csv, dataset_dir)\n",
        "      os.system(mv_file)\n",
        "\n",
        "    return mkdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "miYVQ9vBzRjh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_df(dataset_dir):\n",
        "  \n",
        "    data_path = os.getcwd()+'/'+dataset_dir+'/'\n",
        "\n",
        "    #list files in directory\n",
        "    files = os.listdir(data_path)\n",
        "\n",
        "    #remove non-csv files\n",
        "    while True:\n",
        "        try:\n",
        "            files.remove('.DS_Store')\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    #loop through the files and create the dataframe\n",
        "    df = {}\n",
        "    for file in files:\n",
        "        try:\n",
        "            df[file] = pd.read_csv(data_path+file)\n",
        "        except UnicodeDecodeError:\n",
        "            df[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\") #if utf-8 encoding error\n",
        "        print(file)\n",
        "\n",
        "    #grab names of csv files\n",
        "    df_key = []\n",
        "    for key, value in df.items():\n",
        "        df_key.append(key)\n",
        "\n",
        "    print('loaded datasets to memory')\n",
        "    \n",
        "    return df, df_key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cxhBP3Ez_XA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_tbl_name(filename, schema):\n",
        "  \n",
        "    #rename csv, force lower case, no spaces, no dashes\n",
        "    cleaned_tbl_name = filename.lower().replace(\" \", \"\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\")\n",
        "    \n",
        "    tbl_name = '{0}.{1}'.format(schema, cleaned_tbl_name.split('.')[0]) #save table name and schema\n",
        "\n",
        "    return tbl_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwozDvtF0qfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_colname(df):\n",
        "  \n",
        "    #force column names to be lower case, no spaces, no dashes\n",
        "    df.columns = [x.lower().replace(\" \", \"\").replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\") for x in df.columns]\n",
        "    \n",
        "    #processing data\n",
        "    replacements = {\n",
        "        'timedelta64[ns]': 'varchar',\n",
        "        'object': 'varchar',\n",
        "        'float64': 'float',\n",
        "        'int64': 'int',\n",
        "        'datetime64': 'timestamp'\n",
        "    }\n",
        "\n",
        "    col_str = \", \".join(\"{} {}\".format(n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements)))\n",
        "    \n",
        "    return col_str, df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvqLVLSl23eB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def upload_to_db(schema, host, dbname, user, pwd, tbl_name, col_str, file, dataframe, dataframe_columns):\n",
        "    \n",
        "    #open database connection\n",
        "    conn_string = \"host={0} dbname={1} user={2} password={3}\".format(host, dbname, user, pwd)\n",
        "    conn = psycopg2.connect(conn_string)\n",
        "    cursor = conn.cursor()\n",
        "    print('opened database successfully')\n",
        "        \n",
        "    #execute on db\n",
        "    cursor.execute(\"drop table if exists {0};\".format(tbl_name))\n",
        "    cursor.execute(\"create table %s (%s)\" % (tbl_name, col_str))\n",
        "    print('{0} created successfully'.format(tbl_name))\n",
        "\n",
        "    #save to csv\n",
        "    dataframe.to_csv(k, header = dataframe_columns, index=False, encoding='utf-8') \n",
        "    \n",
        "    #open the file, save as an object, and upload to db\n",
        "    my_file = open(k) \n",
        "    print('file opened in memory')\n",
        "        \n",
        "    SQL_STATEMENT = \"\"\"\n",
        "    COPY %s FROM STDIN WITH\n",
        "        CSV\n",
        "        HEADER\n",
        "        DELIMITER AS ','\n",
        "    \"\"\"\n",
        "    cursor.copy_expert(sql=SQL_STATEMENT % tbl_name, file=my_file)\n",
        "    print('file copied to db')\n",
        "        \n",
        "    cursor.execute(\"grant select on table {0} to public;\".format(tbl_name))\n",
        "    conn.commit()\n",
        "\n",
        "    print('table {0} imported to db'.format(tbl_name))\n",
        "    cursor.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqLBsTznAa6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload files"
      ]
    },
    {
      "metadata": {
        "id": "JDYpvSFCr-9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#upload files to colab server\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nx3KE0WAdhN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ]
    },
    {
      "metadata": {
        "id": "ovXd8T1qqA3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#main\n",
        "\n",
        "#user settings\n",
        "dataset_dir = 'datasets' #folder name to process csv files\n",
        "schema = 'datasets' #postgres db schema name\n",
        "\n",
        "#configure environment and create main df\n",
        "csv_files = csv_files()\n",
        "configure_dataset_directory(csv_files, dataset_dir)\n",
        "df, df_key = create_df(dataset_dir)\n",
        "\n",
        "\n",
        "#loop through all files and upload to db    \n",
        "for k in df_key:\n",
        "    \n",
        "    #call dataframe\n",
        "    dataframe = df[k] \n",
        "\n",
        "    #clean table name\n",
        "    tbl_name = clean_tbl_name(k, schema)\n",
        "    \n",
        "    #clean column names\n",
        "    col_str, dataframe.columns = clean_colname(dataframe)\n",
        "    \n",
        "    \n",
        "    upload_to_db(schema, \n",
        "                 host = 'enter host url', \n",
        "                 dbname = 'enter db name',\n",
        "                 user = 'enter username',\n",
        "                 pwd = 'enter pwd',\n",
        "                 tbl_name = tbl_name, \n",
        "                 col_str = col_str,\n",
        "                 file = k,\n",
        "                 dataframe = dataframe,\n",
        "                 dataframe_columns = dataframe.columns)\n",
        "    \n",
        "        \n",
        "print('all files imported to db')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}